import torch
import os
import time
from transformers import AutoTokenizer, AutoModelForCausalLM

# Disable torch compilation entirely to avoid Triton issues
import torch._dynamo
torch._dynamo.config.disable = True


def load_model(model_path="./pose-llm-gemma"):
    """Load model and tokenizer"""
    print("ü§ñ Gemma Pose Testing")
    print("=" * 50)

    # Check if model exists
    if not os.path.exists(model_path):
        print(f"‚ùå Model not found at: {model_path}")
        print("   Make sure you've trained the model first!")
        return None, None, None

    # Device detection
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"üöÄ GPU detected: {torch.cuda.get_device_name()}")
        print(f"   CUDA version: {torch.version.cuda}")
        print(f"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        device = torch.device("cpu")
        print("‚ö†Ô∏è  GPU not available, using CPU")

    print(f"üéØ Testing device: {device}")
    print()

    try:
        print(f"üì• Loading model: {model_path}")
        start_time = time.time()

        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
        model = AutoModelForCausalLM.from_pretrained(model_path).to(device)

        # Add padding token if not present
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        load_time = time.time() - start_time
        print(f"‚úÖ Model loaded and moved to {device}")
        print(f"‚è±Ô∏è Model loading time: {load_time:.2f} seconds")
        print()

        return model, tokenizer, device
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        return None, None, None


def generate_pose(model, tokenizer, device, description):
    """Generate pose from description using Gemma - with compact array format"""
    # Compact prompt matching the new array-based training data structure
    prompt = f"""Generate a pose JSON for the following description. Use compact array format [x,y,z,w] for rotations and [x,y,z] for positions:

{{"description": "...", "face": {{"ÁúüÈù¢ÁõÆ": 0, "Âõ∞„Çã": 0, "„Å´„Åì„Çä": 0, "ÊÄí„Çä": 0, "„ÅÇ": 0, "„ÅÑ": 0, "„ÅÜ": 0, "„Åà": 0, "„Åä": 0, "„Åä1": 0, "„Å´„ÇÑ„ÇäÔºí": 0, "„Å´„ÇÑ„ÇäÔºí1": 0, "Âè£Ê®™Â∫É„Åí": 0, "Âè£Ê®™Áº©„Åí": 0, "Âè£Ëßí‰∏ä„Åí": 0, "Âè£Ëßí‰∏ã„Åí1": 0, "Âè£Ëßí‰∏ã„Åí": 0, "„Åæ„Å∞„Åü„Åç": 0, "Á¨ë„ÅÑ": 0, "„Ç¶„Ç£„É≥„ÇØ": 0, "„Ç¶„Ç£„É≥„ÇØÂè≥": 0, "„Ç¶„Ç£„É≥„ÇØÔºí": 0, "ÔΩ≥ÔΩ®ÔæùÔΩ∏ÔºíÂè≥": 0, "„Å≥„Å£„Åè„Çä": 0, "ÊÅê„Çç„Åó„ÅÑÂ≠êÔºÅ": 0, "„Å™„Åî„Åø": 0, "„ÅØ„Å°„ÇÖÁõÆ": 0, "„ÅØ„ÅÖ": 0, "ÔΩ∑ÔæòÔΩØ": 0, "ÁúºËßí‰∏ã": 0, "ÁúºÁùë‰∏ä": 0, "„Åò„Å®ÁõÆ": 0, "„Åò„Å®ÁõÆ1": 0, "ÁÖß„Çå": 0}}, "rotatableBones": {{"‰∏äÂçäË∫´": [0, 0, 0, 1], "È¶ñ": [0, 0, 0, 1], "È†≠": [0, 0, 0, 1], "‰∏ãÂçäË∫´": [0, 0, 0, 1], "Â∑¶ËÖï": [0, 0, 0, 1], "Â∑¶„Å≤„Åò": [0, 0, 0, 1], "Â∑¶ÊâãÈ¶ñ": [0, 0, 0, 1], "Â∑¶Ë¶™ÊåáÔºë": [0, 0, 0, 1], "Â∑¶Ë¶™ÊåáÔºí": [0, 0, 0, 1], "Â∑¶‰∫∫ÊåáÔºë": [0, 0, 0, 1], "Â∑¶‰∫∫ÊåáÔºí": [0, 0, 0, 1], "Â∑¶‰∫∫ÊåáÔºì": [0, 0, 0, 1], "Â∑¶‰∏≠ÊåáÔºë": [0, 0, 0, 1], "Â∑¶‰∏≠ÊåáÔºí": [0, 0, 0, 1], "Â∑¶‰∏≠ÊåáÔºì": [0, 0, 0, 1], "Â∑¶Ëñ¨ÊåáÔºë": [0, 0, 0, 1], "Â∑¶Ëñ¨ÊåáÔºí": [0, 0, 0, 1], "Â∑¶Ëñ¨ÊåáÔºì": [0, 0, 0, 1], "Â∑¶Â∞èÊåáÔºë": [0, 0, 0, 1], "Â∑¶Â∞èÊåáÔºí": [0, 0, 0, 1], "Â∑¶Â∞èÊåáÔºì": [0, 0, 0, 1], "Â∑¶Ë∂≥": [0, 0, 0, 1], "Â∑¶„Å≤„Åñ": [0, 0, 0, 1], "Â∑¶Ë∂≥È¶ñ": [0, 0, 0, 1], "Âè≥ËÖï": [0, 0, 0, 1], "Âè≥„Å≤„Åò": [0, 0, 0, 1], "Âè≥ÊâãÈ¶ñ": [0, 0, 0, 1], "Âè≥Ë¶™ÊåáÔºë": [0, 0, 0, 1], "Âè≥Ë¶™ÊåáÔºí": [0, 0, 0, 1], "Âè≥‰∫∫ÊåáÔºë": [0, 0, 0, 1], "Âè≥‰∫∫ÊåáÔºí": [0, 0, 0, 1], "Âè≥‰∫∫ÊåáÔºì": [0, 0, 0, 1], "Âè≥‰∏≠ÊåáÔºë": [0, 0, 0, 1], "Âè≥‰∏≠ÊåáÔºí": [0, 0, 0, 1], "Âè≥‰∏≠ÊåáÔºì": [0, 0, 0, 1], "Âè≥Ëñ¨ÊåáÔºë": [0, 0, 0, 1], "Âè≥Ëñ¨ÊåáÔºí": [0, 0, 0, 1], "Âè≥Ëñ¨ÊåáÔºì": [0, 0, 0, 1], "Âè≥Â∞èÊåáÔºë": [0, 0, 0, 1], "Âè≥Â∞èÊåáÔºí": [0, 0, 0, 1], "Âè≥Â∞èÊåáÔºì": [0, 0, 0, 1], "Âè≥Ë∂≥": [0, 0, 0, 1], "Âè≥„Å≤„Åñ": [0, 0, 0, 1]}}, "movableBones": {{"„Çª„É≥„Çø„Éº": [0, 7.95, -0.45], "Â∑¶Ë∂≥Ôº©Ôº´": [-0.5, 0.8, -1.2], "Âè≥Ë∂≥Ôº©Ôº´": [0.35, 0.8, 0], "Â∑¶„Å§„ÅæÂÖàÔº©Ôº´": [0, -1.59, -1.82], "Âè≥„Å§„ÅæÂÖàÔº©Ôº´": [0, -1.59, -1.82]}}}}

Description: {description}
JSON: """

    print(f"üìù Input prompt:")
    print(f"Description: {description}")
    print("JSON template provided with compact array format")
    print("-" * 60)

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(device)
    input_length = inputs['input_ids'].shape[1]

    print(f"üî¢ Input tokens: {input_length}")

    # Start timing generation
    start_time = time.time()

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=800,  # Reduced since compact format needs fewer tokens
            do_sample=True,
            temperature=0.2,  # Lower temperature for more structured output
            top_p=0.8,  # Add top_p for better control
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.2  # Add repetition penalty to prevent loops
        )

    generation_time = time.time() - start_time

    # Decode only the generated part
    generated_tokens = outputs[0][input_length:]
    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    print(f"‚è±Ô∏è Generation time: {generation_time:.2f} seconds")
    print(f"üî¢ Generated tokens: {len(generated_tokens)}")
    print(f"‚ö° Tokens per second: {len(generated_tokens)/generation_time:.1f}")
    print(f"üì§ Generated text (full):")
    print(generated_text)  # Show full text, no truncation

    return generated_text


def quick_test(model, tokenizer, device):
    """Quick test with one training example"""
    print("üß™ Quick validation test:")
    print("-" * 40)

    # Test with one description from training data
    description = "Standing with both arms extended forward"
    result = generate_pose(model, tokenizer, device, description)

    print(f"\n‚úÖ Result length: {len(result)} characters")
    if len(result) > 10 and "{" in result:
        print("‚úÖ Generated JSON-like content!")

        # Try to parse as JSON to validate structure
        try:
            import json
            # Find the JSON part (starts with {)
            json_start = result.find('{')
            if json_start != -1:
                json_part = result[json_start:]
                # Try to find end of JSON
                brace_count = 0
                json_end = json_start
                for i, char in enumerate(json_part):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            json_end = json_start + i + 1
                            break

                if json_end > json_start:
                    json_text = result[json_start:json_end]
                    parsed = json.loads(json_text)
                    print("‚úÖ Valid JSON structure!")
                    print(f"üìä JSON keys: {list(parsed.keys())}")
        except Exception as e:
            print(f"‚ö†Ô∏è JSON parsing issue: {e}")
    else:
        print("‚ùå No meaningful JSON generated")

    return result


def test_training_data(model, tokenizer, device):
    """Test with descriptions from training data"""
    training_descriptions = [
        "Standing with both arms extended forward",
        "Sitting with hands on knees",
        "Walking forward with normal gait",
        "Waving with right hand"
    ]

    print("üß™ Testing with training data descriptions:")
    print("=" * 60)

    for i, desc in enumerate(training_descriptions, 1):
        print(f"\nüìã Test {i}: {desc}")
        print("-" * 40)

        result = generate_pose(model, tokenizer, device, desc)

        print(f"‚úÖ Result {i}:")
        print(result[:200] + "..." if len(result) > 200 else result)
        print("-" * 60)


def main():
    # Load model
    model, tokenizer, device = load_model()

    if model is None:
        print("‚ùå Cannot proceed without model")
        return

    # Quick validation test
    quick_test(model, tokenizer, device)

    print("‚úÖ Test complete!")


if __name__ == "__main__":
    main()
